# Must Read Natural Language Processing Papers

![Banner](https://github.com/theepiccode/TEC-Assets/blob/main/Images/Big-Banner.png?raw=true)
<p align="left"> <img src="https://komarev.com/ghpvc/?username=theepiccode&label=Views&color=blue&style=plastic" alt="theepiccode" /> </p>
<a href = "https://invite.theepiccode.com" align = "left">
<img src = "https://img.shields.io/badge/Discord-Join%20the%20Server-blue" /> 
</a>
<br>
<br>
<a href="https://twitter.com/theepiccode1">
  <img align="left" alt="theepiccode's Twitter" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/twitter.svg" />
</a>
<a href="https://www.linkedin.com/company/theepiccode/">
  <img align="left" alt="theepiccode's Linkdein" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg" />
</a>
<a href="https://github.com/theepiccode">
  <img align="left" alt="theepiccode's Github" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/github.svg" />
</a>
<a href="https://www.instagram.com/theepiccode/">
  <img align="left" alt="theepiccode's Instagram" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/instagram.svg" />
</a>
<a href="https://www.youtube.com/theepiccode">
  <img align="left" alt="theepiccode's Youtube" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/youtube.svg" />
</a>
<br>
<br>


# Table of Content

* [Distributed Word Representations](#distributed-word-representations)
* [Distributed Sentence Representations](#distributed-sentence-representations)
* [Entity Recognition](#entity-recognition)
* [Language Model](#language-model)
* [Machine Translation](#machine-translation)
* [Question Answering](#question-answering)
* [Recommendation Systems](#recommendation-systems)
* [Relation Extraction](#relation-extraction)
* [Sentence Matching](#sentence-matching)
* [Text Classification](#text-classification)

## Distributed Word Representations

* **Improving vector space word representations using multilingual correlation**. Faruqui and Dyer .(2014) [[pdf](https://www.aclweb.org/anthology/E14-1049/)]
* **Visualising data using t-SNE**. Maaten and Hinton 2008 [[pdf](https://www.jmlr.org/papers/v9/vandermaaten08a.html)]
* **Finding function in form: Compositional character models for open vocabulary word representation**. Ling et al. (2015) [[pdf](https://arxiv.org/abs/1508.02096)]
* **Enriching word vectors with subword information**. Bojanowski et al. (2016) [[pdf](https://arxiv.org/abs/1607.04606)]
* **Quick Training of Probabilistic Neural Nets by Importance Sampling**. Bengio and Sen√©cal (2003) [[pdf](http://www.iro.umontreal.ca/~lisa/pointeurs/senecal_aistats2003.pdf)]

## Distributed Sentence Representations

* **A Model of Coherence Based on Distributed Sentence Representation**. Li and Hovy (2014) [[pdf](https://www.aclweb.org/anthology/D14-1218/)]
* **Distributed representations of sentences and documents**. Le and Mikolov (2014) [[pdf](http://proceedings.mlr.press/v32/le14.pdf)]
* **Skip-Thought Vectors**. Kiros et al (2015) [[pdf](http://papers.nips.cc/paper/5950-skip-thought-vectors)]
* **Learning Distributed Representations of Sentences from Unlabelled Data** Hill et al (2016) [[pdf](https://arxiv.org/abs/1602.03483)]
* **A simple but tough-to-beat baseline for sentence embeddings**. Arora et al (2016) [[pdf](https://openreview.net/forum?id=SyK00v5xx)]
* **Unsupervised Learning of Sentence Embeddings using Compositional n-Gram Features (sent2vec)** Pagliardini et al (2017) [[pdf](https://arxiv.org/abs/1703.02507)]
* **An efficient framework for learning sentence representations (Quick-Thought Vectors)** Logeswaran et al. (2018) [[pdf](https://arxiv.org/abs/1803.02893)]
* **Towards universal paraphrastic sentence embeddings** Wieting et al. (2015) [[pdf](https://arxiv.org/abs/1511.08198)]
* **Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks** Adi et al. (2016) [[pdf](https://arxiv.org/abs/1608.04207)]
* **Supervised Learning of Universal Sentence Representations from Natural Language Inference Data (InferSent)** Conneau et al. (2017) [[pdf](https://arxiv.org/abs/1705.02364)]
* **Universal Sentence Encoder** Cer et al. (2018) [[pdf](https://arxiv.org/abs/1803.11175)]

## Entity Recognition

* **Neural Architectures for Named Entity Recognition** Lample et al. (2016) [[pdf](https://arxiv.org/abs/1603.01360)]
* **End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF** Ma and Hovy (2016) [[pdf](https://arxiv.org/abs/1603.01354)]
* **Transfer Learning for Sequence Tagging with Hierarchical Recurrent Networks** Yang et al. (2017) [[pdf](https://arxiv.org/abs/1703.06345)]
* **Semi-supervised sequence tagging with bidirectional language models** Peters et al. (2017) [[pdf](https://arxiv.org/abs/1705.00108)]
* **Learning Named Entity Tagger using Domain-Specific Dictionary** Shang et al. (2018) [[pdf](https://arxiv.org/abs/1809.03599)]


## Language Model

* **A neural probabilistic language model** Bengio et al. (2003) [[pdf](https://www.jmlr.org/papers/v3/bengio03a.html)]
* **Using the output embedding to improve language model** Press and Wolf (2016) [[pdf](https://arxiv.org/abs/1608.05859)]
* **Deep contextualized word representations(ELMo)** Peters et al. (2018) [[pdf](https://arxiv.org/abs/1802.05365)]
* **Universal language model fine-tuning for text classification(ULMFit)** Howard and Ruder (2018) [[pdf](https://www.aclweb.org/anthology/P18-1031/)]
* **Improving language understanding by generative pre-training** Radford et al. (2018) [[pdf](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)]
* **Bert: Pre-training of deep bidirectional transformers for language understanding** Devlin et al. (2018) [[pdf](https://arxiv.org/abs/1810.04805)]

## Machine Translation 

* **Neural Machine Translation via Binary Code Predict** Oda et al. (2017) [[pdf](https://arxiv.org/abs/1704.06918)]
* **Neural machine translation in linear time** Kalchbrenner et al. (2016) [[pdf](https://arxiv.org/abs/1610.10099)]
* **Sequence to Sequence Learning with Neural Networks** Sutskever et al. (2014) [[pdf](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html)]
* **Learning Phrase Representations using RNN Encoder-Decoder for NMT** Cho et al. (2014) [[pdf](https://arxiv.org/abs/1406.1078)]
* **NMT by Jointly Learning to Align and Translate** Bahdanau et al. (2014) [[pdf](https://arxiv.org/abs/1409.0473)]
* **Effective Approaches to Attention-based NMT** Luong et al. (2015) [[pdf](https://arxiv.org/abs/1508.04025)]
* **Convolutional sequence to sequence learning** Gehring et al. (2017) [[pdf](https://arxiv.org/abs/1705.03122)]
* **Attention is all you need** Vaswani et al. (2017) [[pdf](https://arxiv.org/abs/1706.03762)]

## Question Answering

* **Machine Comprehension Using Match-LSTM and Answer Pointer** Wang and Jiang. (2016) [[pdf](https://arxiv.org/abs/1608.07905)]
* **Bidirectional Attention Flow for Machine Comprehension** Seo et al. (2016) [[pdf](https://arxiv.org/abs/1611.01603)]
* **Attention-over-Attention Neural Networks for Reading Comprehension** Cui et al. (2016) [[pdf](https://arxiv.org/abs/1607.04423)]
* **Simple and Effective Multi-Paragraph Reading Comprehension** Clark and Gardner. (2017) [[pdf](https://arxiv.org/abs/1710.10723)]
* **Gated Self-Matching Networks for Reading Comprehension and Question Answering** Wang et al. (2017) [[pdf](https://www.aclweb.org/anthology/P17-1018/)]
* **QANet: Combining Local Convolution with Global Self-Attention for Reading Comprehension** Yu et al. (2018) [[pdf](https://arxiv.org/abs/1804.09541)]

## Recommendation Systems

* **Factorization machines** Rendle S. (2010) [[pdf](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=5694074)]
* **Wide & Deep Learning for Recommender Systems** Cheng et al. (2016) [[pdf](https://dl.acm.org/doi/10.1145/2988450.2988454)]
* **DeepFM: A Factorization-Machine based Neural Network for CTR Prediction** Guo et al. (2017) [[pdf](https://arxiv.org/abs/1703.04247)]
* **Neural Factorization Machines for Sparse Predictive Analytics** He and Chua. (2017) [[pdf](https://dl.acm.org/doi/10.1145/3077136.3080777)]

## Relation Extraction

* **Distant supervision for relation extraction without labeled data** Mintz et al. (2009)[[pdf](https://dl.acm.org/doi/10.5555/1690219.1690287)]
* **Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks** Zeng et al. (2015) [[pdf](https://www.aclweb.org/anthology/D15-1203/)]
* **Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification** Zhou et al. (2016) [[pdf](https://www.aclweb.org/anthology/P16-2034/)]
* **Neural Relation Extraction with Selective Attention over Instances** Lin et al. (2016) [[pdf](https://www.aclweb.org/anthology/P16-1200/)]
* **Distant Supervision for Relation Extraction with Sentence-Level Attention and Entity Descriptions** Ji et al. (2017) [[pdf](http://www.aaai.org/ocs/index.php/AAAI/AAAI17/paper/download/14491/14078)]

## Sentences Matching

* **Convolutional neural network architectures for Matching Natural Language Sentences** Hu et al. (2014) [[pdf](https://papers.nips.cc/paper/2014/file/b9d487a30398d42ecff55c228ed5652b-Paper.pdf)]
* **Shortcut-Stacked Sentence Encoders for Multi-Domain Inference** Nie and Bansal (2017) [[pdf](https://arxiv.org/abs/1708.02312)]
* **Bilateral Multi-Perspective Matching for Natural Language Sentences** Wang et al. (2017) [[pdf](https://arxiv.org/abs/1702.03814)]
* **A Compare-Propagate Architecture with Alignment Factorization for Natural Language Inference** Tay et al. (2017) [[pdf](https://arxiv.org/abs/1801.00102)]
* **Enhanced LSTM for Natural Language Inference** Chen et al. (2017) [[pdf](https://arxiv.org/abs/1609.06038)]
* **DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference** Ghaeini et al. (2018) [[pdf](https://arxiv.org/abs/1802.05577)]

## Text Classification

* **Bag of tricks for efficient text classification** Joulin et al. (2016) [[pdf](https://arxiv.org/abs/1607.01759v3)]
* **Convolutional neural networks for sentence classification** Kim (2014) [[pdf](https://arxiv.org/abs/1408.5882)]
* **A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification**. Zhang and Wallace (2015) [[pdf](https://arxiv.org/abs/1510.03820)]
* **Character-level convolutional networks for text classification** Zhang et al. (2015) [[pdf](https://proceedings.neurips.cc/paper/2015/hash/250cf8b51c773f3f8dc8b4be867a9a02-Abstract.html)]
* **Recurrent Convolutional Neural Networks for Text Classification** Lai et al. (2015) [[pdf](http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf)]
* **Hierarchical attention networks for document classification** Yang et al. (2016) [[pdf](https://www.aclweb.org/anthology/N16-1174/)]
* **Deep unordered composition rivals syntactic methods for Text Classification** Iyyer et al (2015) [[pdf](https://www.aclweb.org/anthology/P15-1162/)]
* **Attention-based LSTM for aspect-level sentiment classification**. Wang et al (2016) [[pdf](https://www.aclweb.org/anthology/D16-1058/)]
* **Aspect level sentiment classification with deep memory network** Tang et al. (2016) [[pdf](https://arxiv.org/abs/1605.08900)]
* **Recurrent Attention Network on Memory for Aspect Sentiment Analysis** Chen et al. (2017) [[pdf](https://www.aclweb.org/anthology/D17-1047/)]
* **Aspect Based Sentiment Analysis with Gated Convolutional Networks** Xue and Li (2018) [[pdf](https://arxiv.org/abs/1805.07043)]
